{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, getopt \n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import yaml\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import keras\n",
    "from matplotlib import pyplot as plt \n",
    "import keras.callbacks\n",
    "from keras.datasets import mnist\n",
    "#from keras.utils.visualize_util import plot\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from model import build_model, MyCallback\n",
    "from keras.callbacks import CSVLogger\n",
    "from data import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "#np.set_printoptions(threshold=10000000)\n",
    "from theano.printing import return_value,reset_value\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '803262'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_data_dict(in_name, out_name, in_data, out_data):\n",
    "    in_dict = dict()\n",
    "    in_dict[in_name] = in_data\n",
    "    \n",
    "    out_dict = dict((k, out_data) for k in out_name)\n",
    "    return (in_dict, out_dict)\n",
    "\n",
    "def build_loss_weight(config):\n",
    "    if config['hedge'] == False:\n",
    "        w = [1.]\n",
    "    elif config['loss_weight'] == 'ave':\n",
    "        w = [1./ config['n_layers']]* config['n_layers']\n",
    "    return w\n",
    "def eer(y_true, y_pred):\n",
    "    y_true.eval()\n",
    "    #print type(K.mean(y_pred))\n",
    "    x1 = T.dscalar('x')\n",
    "    y1 = T.dscalar('x')\n",
    "    f = function([x], z)\n",
    "    \n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    \n",
    "    '''Calculates the mean accuracy rate across all predictions for binary\n",
    "    classification problems.\n",
    "    '''\n",
    "    if print_mode==1:\n",
    "        y_pred = K.print_tensor(y_pred)\n",
    "    #K.print_tensor(y_pred)\n",
    "    #K.get_value(y_pred)\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(arg, idx=0):\n",
    "    config = {'learning_rate': 1e-3,\n",
    "              'optim': 'Adam',\n",
    "              'batch_size': 1,\n",
    "              'nb_epoch': 50,\n",
    "              'n_layers': 10,\n",
    "              'hidden_num': 100,\n",
    "              'activation': 'relu',\n",
    "              'loss_weight': 'ave',\n",
    "              'adaptive_weight': False,\n",
    "              'data': 'mnist',\n",
    "              'hedge': True,\n",
    "              'log': 'mnist_hedge.log'}\n",
    "\n",
    "    configfile = ''\n",
    "    opts = [('-c', 'hb19.yaml')]\n",
    "    args = []\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print (helpstring)\n",
    "            yamlstring = yaml.dump(config,default_flow_style=False,explicit_start=True)\n",
    "            print(\"YAML configuration file format:\")\n",
    "            print(\"\")\n",
    "            print(\"%YAML 1.2\")\n",
    "            print(yamlstring)\n",
    "            sys.exit()\n",
    "\n",
    "        elif opt in ('-c', '--config'):\n",
    "            configfile = arg\n",
    "\n",
    "        print(\"Config file is %s\" % configfile)\n",
    "\n",
    "    if os.path.exists(configfile):\n",
    "        f = open(configfile)\n",
    "        user_config = yaml.load(f.read())\n",
    "        config.update(user_config)\n",
    "    \n",
    "    print(\"Printing configuration:\")\n",
    "    for key,value in config.iteritems():\n",
    "        print(\"  \",key,\": \",value)\n",
    "        \n",
    "    (X_train, Y_train, X_test, Y_test, nb_classes) = load(dataset)\n",
    "    (_, _, x_test, y_test, nb_classes) = load(dataset)\n",
    "    \n",
    "    print np.array_equal(Y_test,y_test)\n",
    "    \n",
    "    Y_TRAIN = Y_train.reshape(-1)\n",
    "    Y_TEST = Y_test.reshape(-1)\n",
    "    \n",
    "    train_len = Y_TRAIN.shape[0]\n",
    "    test_len = Y_TEST.shape[0]\n",
    "    \n",
    "    Y_train = np.zeros((train_len,2))\n",
    "    Y_test = np.zeros((test_len,2))\n",
    "    \n",
    "    Y_train[np.arange(train_len), Y_TRAIN] = 1\n",
    "    Y_test[np.arange(test_len), Y_TEST] = 1\n",
    "    \n",
    "    \n",
    "    #(X_test, Y_test, X_train, Y_train, nb_classes) = load(config['data'])\n",
    "    #print Y_test\n",
    "    #X_train = X_train[:500000]\n",
    "    #Y_train = Y_train[:500000]\n",
    "    '''\n",
    "    X_test = X_test[:1]\n",
    "    Y_test = Y_test[:1]'''\n",
    "    '''\n",
    "    # data for concept drift data \n",
    "    import numpy as np\n",
    "\n",
    "    X = np.load('/home/rakesh/Desktop/zCap/x_train.npy')\n",
    "    y = np.load('/home/rakesh/Desktop/zCap/y_train.npy')\n",
    "    \n",
    "    X_test, X_train, Y_test, Y_train = train_test_split(X, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.211)\n",
    "    '''\n",
    "    model, in_name, out_name = build_model(config)\n",
    "    if len(out_name) == 1:\n",
    "        out_name_loss = ['loss']\n",
    "    else:\n",
    "        out_name_loss = [s + '_loss' for s in out_name]\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    #print Y_test[:,1].sum()\n",
    "    \n",
    "    #plot(model, to_file = 'model.png')\n",
    "    \n",
    "    optim = eval(config['optim'])(lr = config['learning_rate'])\n",
    "    in_dict, out_dict = build_data_dict(in_name, out_name, X_train, Y_train)\n",
    "    in_val, out_val = build_data_dict(in_name, out_name, X_test, Y_test)\n",
    "    loss_dict = dict((k, 'categorical_crossentropy') for k in out_name) \n",
    "  \n",
    "    loss_weights = build_loss_weight(config)\n",
    "    my_callback = MyCallback( loss_weights, names = out_name_loss, hedge = config['hedge'], log_name = config['log'])\n",
    "    #csv  = CSVLogger(config['log'])\n",
    "    model.compile(optimizer = optim, loss = loss_dict, hedge = config['hedge'],loss_weights = loss_weights, metrics =['accuracy'])#,binary_accuracy]) #['accuracy'])\n",
    "    history = model.fit(in_dict, out_dict,validation_split=0.2, nb_epoch = config['nb_epoch'], batch_size = config['batch_size'], callbacks=[my_callback] , verbose = 1)\n",
    "    #history = model.fit(in_val, out_val, nb_epoch = 1, batch_size = 10, callbacks=[my_callback] , verbose = 1)\n",
    "    #print(len(k))\n",
    "    #print model.evaluate(X_test,Y_test)\n",
    "    #print my_callback.acc\n",
    "    \n",
    "    k=return_value()\n",
    "    #print(k[0].shape)\n",
    "    cumLoss = np.cumsum(my_callback.acc)\n",
    "    indexOfLoss = np.arange(len(cumLoss))+1\n",
    "    cumAverageLoss = cumLoss/indexOfLoss\n",
    "    filename = (config['log'] + '_' + str(idx) + '.acc')\n",
    "    np.savetxt(filename, cumAverageLoss, delimiter=',')\n",
    "    \n",
    "    return my_callback,history,model,x_test,y_test,in_val,out_val,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = None\n",
    "Model1 = None\n",
    "X_test = None\n",
    "Y_test = None\n",
    "in_val = None\n",
    "out_val = None\n",
    "my_callback = None\n",
    "k=None\n",
    "if __name__ == '__main__':\n",
    "    #for i in range(5):\n",
    "    my_callback,history,Model1,X_test,Y_test,in_val,out_val,k = main(sys.argv[1:], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_value()                    ## clears prediction buffer\n",
    "print return_value()                            \n",
    "metrics = Model1.evaluate(in_val,out_val,verbose=1)\n",
    "k=return_value()                                  # predictions from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test metrics\n",
    "print(\"Test Accuracy : \",metrics[-1])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate(k)\n",
    "y_true = out_val[out_val.keys()[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = y_pred[:,1]\n",
    "y_true = y_true[:,1]\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "fnr = 1 - tpr\n",
    "EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"FPR\",fpr.shape\n",
    "print \"TPR\",tpr.shape\n",
    "print \"thresholds\",thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"AUC\",roc_auc_score(y_true, y_scores)\n",
    "print \"EER\",EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnr[np.nanargmin(np.absolute((fnr - fpr)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "thresh = interp1d(fpr, thresholds)(eer)\n",
    "print \"EER\",eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.gcf() # get current figure\n",
    "figure.set_size_inches(8, 6)\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_score(y_true, y_scores))\n",
    "\n",
    "plt.plot(fpr, tpr, 'b', label = 'EER = %0.2f' % eer)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.02, 1.02])\n",
    "plt.ylim([-0.02, 1.02])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(dataset+' ROC - 50ms ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(dataset+'_plot_hbp_roc.png',dpi=100)\n",
    "\n",
    "plt.clf() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error rate whil whole training\n",
    "\n",
    "hbp = open('/home/rakesh/Desktop/zCap/new/log_hbp19_0.acc','r')\n",
    "\n",
    "xhbp = []\n",
    "\n",
    "for line in hbp.readlines():\n",
    "    xhbp.append(float(line))\n",
    "    \n",
    "    \n",
    "figure = plt.gcf() # get current figure\n",
    "figure.set_size_inches(8, 6)\n",
    "\n",
    "plt.plot(1-np.array(xhbp), label = 'HBP')\n",
    "plt.legend()\n",
    "plt.title(dataset+' 50ms 21 features HBP')\n",
    "plt.ylabel('Error Rate')\n",
    "#plt.show()\n",
    "plt.savefig(dataset+'_plot_hbp_cd.png',dpi=100)\n",
    "\n",
    "plt.clf()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##v  train and val curve\n",
    "figure = plt.gcf() # get current figure\n",
    "figure.set_size_inches(8, 6)\n",
    "\n",
    "plt.plot(np.array(history.history['weighted_acc']), label = 'Training')\n",
    "plt.plot(np.array(history.history['val_weighted_acc']), label = 'Validation')\n",
    "plt.legend()\n",
    "plt.title(dataset+' | 50ms | Test_acc : %0.3f'%metrics[-1])\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.show()\n",
    "plt.savefig(dataset+'_plot_train_val.png',dpi=100)\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
